{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Pipeline - Home Credit Default Risk\n",
    "\n",
    "This notebook trains multiple models with hyperparameter tuning and MLflow tracking:\n",
    "- Logistic Regression (baseline)\n",
    "- Random Forest\n",
    "- XGBoost (GPU-enabled on Mac if available)\n",
    "- LightGBM (GPU-enabled on Mac if available)\n",
    "\n",
    "All experiments are logged to MLflow server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import platform\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "\n",
    "# Import custom modules - force reload\n",
    "import importlib\n",
    "if 'metrics' in sys.modules:\n",
    "    importlib.reload(sys.modules['metrics'])\n",
    "if 'model_utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['model_utils'])\n",
    "\n",
    "from model_utils import *\n",
    "from metrics import *\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect GPU/Device Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect platform and GPU availability\n",
    "is_mac = platform.system() == 'Darwin'\n",
    "is_apple_silicon = is_mac and platform.machine() == 'arm64'\n",
    "\n",
    "print(f\"Platform: {platform.system()}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Apple Silicon: {is_apple_silicon}\")\n",
    "\n",
    "# Configure device settings\n",
    "if is_apple_silicon:\n",
    "    # Apple Silicon (M1/M2/M3) - try to use GPU\n",
    "    xgb_device = 'cpu'  # XGBoost GPU support on Mac is experimental\n",
    "    lgb_device = 'cpu'  # LightGBM GPU requires OpenCL setup\n",
    "    print(\"\\nNote: Using CPU for training (GPU support on Mac requires special builds)\")\n",
    "    print(\"All models will use n_jobs=-1 for maximum CPU utilization\")\n",
    "else:\n",
    "    xgb_device = 'cpu'\n",
    "    lgb_device = 'cpu'\n",
    "    print(\"\\nUsing CPU with multi-threading (n_jobs=-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure MLflow (Server Connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MLflow server on localhost:5000\n",
    "mlflow_server_uri = \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(mlflow_server_uri)\n",
    "\n",
    "# Create or set experiment\n",
    "experiment_name = \"home_credit_default_risk\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    print(f\"Created new experiment: {experiment_name}\")\n",
    "except:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"Using existing experiment: {experiment_name}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"\\nMLflow Configuration:\")\n",
    "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"  Experiment: {experiment_name}\")\n",
    "print(f\"  Experiment ID: {experiment_id}\")\n",
    "print(f\"\\n‚úì Connected to MLflow server!\")\n",
    "print(f\"  View UI at: {mlflow_server_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared data from notebook 01\n",
    "print(\"Loading prepared data...\")\n",
    "df = pd.read_csv('../data/application_train_prepared.csv')\n",
    "\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "if 'TARGET' in df.columns:\n",
    "    target_col = 'TARGET'\n",
    "else:\n",
    "    target_col = [col for col in df.columns if 'target' in col.lower()][0]\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df[target_col].value_counts(normalize=True))\n",
    "\n",
    "# Calculate imbalance ratio for scale_pos_weight\n",
    "n_neg = (df[target_col] == 0).sum()\n",
    "n_pos = (df[target_col] == 1).sum()\n",
    "scale_pos_weight = n_neg / n_pos\n",
    "print(f\"\\nScale pos weight (for XGBoost): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[target_col].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0=No Default, 1=Default)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Stratified split to maintain class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Feature scaling for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Save scaler\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"\\n‚úì Scaler saved to ../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model and return metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics with default threshold\n",
    "    results = calculate_all_metrics(y_test, y_pred_proba, threshold=0.5)\n",
    "    \n",
    "    # Optimize threshold for business cost\n",
    "    optimal_threshold = optimize_threshold(\n",
    "        y_test, \n",
    "        y_pred_proba, \n",
    "        metric='business_cost',\n",
    "        fn_cost=1,\n",
    "        fp_cost=10\n",
    "    )\n",
    "    \n",
    "    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    business_cost = calculate_business_cost(y_test, y_pred_optimal, fn_cost=1, fp_cost=10)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  AUC:                 {results['auc']:.4f}\")\n",
    "    print(f\"  Accuracy:            {results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision:           {results['precision']:.4f}\")\n",
    "    print(f\"  Recall:              {results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:            {results['f1']:.4f}\")\n",
    "    print(f\"  Optimal Threshold:   {optimal_threshold:.4f}\")\n",
    "    print(f\"  Business Cost:       {business_cost:.2f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return {\n",
    "        **results,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'business_cost_optimal': business_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Logistic Regression (Baseline)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_params = {\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 1000,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "lr_model = LogisticRegression(**lr_params)\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"\\nRunning 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"CV AUC scores: {cv_scores}\")\n",
    "print(f\"Mean CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Train final model\n",
    "print(\"\\nTraining final model...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics = evaluate_model(lr_model, X_test_scaled, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"logistic_regression_baseline\"):\n",
    "    mlflow.log_params(lr_params)\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"device\", \"cpu\")\n",
    "    \n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_scores.mean())\n",
    "    mlflow.log_metric(\"cv_auc_std\", cv_scores.std())\n",
    "    mlflow.log_metrics({\n",
    "        \"test_auc\": lr_metrics['auc'],\n",
    "        \"test_accuracy\": lr_metrics['accuracy'],\n",
    "        \"test_precision\": lr_metrics['precision'],\n",
    "        \"test_recall\": lr_metrics['recall'],\n",
    "        \"test_f1\": lr_metrics['f1'],\n",
    "        \"optimal_threshold\": lr_metrics['optimal_threshold'],\n",
    "        \"business_cost\": lr_metrics['business_cost_optimal']\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(lr_model, \"model\")\n",
    "    print(\"\\n‚úì Logged to MLflow!\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(lr_model, '../models/logistic_regression.pkl')\n",
    "print(\"‚úì Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Random Forest with RandomizedSearchCV\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced'],\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter tuning (20 iterations, 3-fold CV)...\")\n",
    "rf_base = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(\n",
    "    rf_base, rf_param_grid,\n",
    "    n_iter=20, cv=3, scoring='roc_auc',\n",
    "    n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best CV AUC: {rf_random.best_score_:.4f}\")\n",
    "\n",
    "# Train final model with 5-fold CV\n",
    "rf_model = RandomForestClassifier(**rf_random.best_params_)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"\\n5-Fold CV AUC: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"random_forest_tuned\"):\n",
    "    mlflow.log_params(rf_random.best_params_)\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"n_iter_random_search\", 20)\n",
    "    mlflow.log_param(\"device\", \"cpu\")\n",
    "    \n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_scores_rf.mean())\n",
    "    mlflow.log_metric(\"cv_auc_std\", cv_scores_rf.std())\n",
    "    mlflow.log_metrics({\n",
    "        \"test_auc\": rf_metrics['auc'],\n",
    "        \"test_accuracy\": rf_metrics['accuracy'],\n",
    "        \"test_precision\": rf_metrics['precision'],\n",
    "        \"test_recall\": rf_metrics['recall'],\n",
    "        \"test_f1\": rf_metrics['f1'],\n",
    "        \"optimal_threshold\": rf_metrics['optimal_threshold'],\n",
    "        \"business_cost\": rf_metrics['business_cost_optimal']\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(rf_model, \"model\")\n",
    "    print(\"\\n‚úì Logged to MLflow!\")\n",
    "\n",
    "joblib.dump(rf_model, '../models/random_forest.pkl')\n",
    "print(\"‚úì Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. XGBoost (with device configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training XGBoost with RandomizedSearchCV\")\n",
    "print(f\"Device: {xgb_device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'scale_pos_weight': [scale_pos_weight],\n",
    "    'tree_method': ['hist'],  # Fast histogram-based method\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter tuning (20 iterations, 3-fold CV)...\")\n",
    "xgb_base = xgb.XGBClassifier()\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    xgb_base, xgb_param_grid,\n",
    "    n_iter=20, cv=3, scoring='roc_auc',\n",
    "    n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters: {xgb_random.best_params_}\")\n",
    "print(f\"Best CV AUC: {xgb_random.best_score_:.4f}\")\n",
    "\n",
    "# Train final model\n",
    "xgb_model = xgb.XGBClassifier(**xgb_random.best_params_)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"\\n5-Fold CV AUC: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std() * 2:.4f})\")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics = evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"xgboost_tuned\"):\n",
    "    mlflow.log_params(xgb_random.best_params_)\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"n_iter_random_search\", 20)\n",
    "    mlflow.log_param(\"device\", xgb_device)\n",
    "    \n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_scores_xgb.mean())\n",
    "    mlflow.log_metric(\"cv_auc_std\", cv_scores_xgb.std())\n",
    "    mlflow.log_metrics({\n",
    "        \"test_auc\": xgb_metrics['auc'],\n",
    "        \"test_accuracy\": xgb_metrics['accuracy'],\n",
    "        \"test_precision\": xgb_metrics['precision'],\n",
    "        \"test_recall\": xgb_metrics['recall'],\n",
    "        \"test_f1\": xgb_metrics['f1'],\n",
    "        \"optimal_threshold\": xgb_metrics['optimal_threshold'],\n",
    "        \"business_cost\": xgb_metrics['business_cost_optimal']\n",
    "    })\n",
    "    \n",
    "    mlflow.xgboost.log_model(xgb_model, \"model\")\n",
    "    print(\"\\n‚úì Logged to MLflow!\")\n",
    "\n",
    "joblib.dump(xgb_model, '../models/xgboost.pkl')\n",
    "print(\"‚úì Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. LightGBM (with device configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training LightGBM with RandomizedSearchCV\")\n",
    "print(f\"Device: {lgb_device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'class_weight': ['balanced'],\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter tuning (20 iterations, 3-fold CV)...\")\n",
    "lgb_base = lgb.LGBMClassifier()\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    lgb_base, lgb_param_grid,\n",
    "    n_iter=20, cv=3, scoring='roc_auc',\n",
    "    n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "\n",
    "lgb_random.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters: {lgb_random.best_params_}\")\n",
    "print(f\"Best CV AUC: {lgb_random.best_score_:.4f}\")\n",
    "\n",
    "# Train final model\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_random.best_params_)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_lgb = cross_val_score(lgb_model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"\\n5-Fold CV AUC: {cv_scores_lgb.mean():.4f} (+/- {cv_scores_lgb.std() * 2:.4f})\")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lgb_metrics = evaluate_model(lgb_model, X_test, y_test, \"LightGBM\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"lightgbm_tuned\"):\n",
    "    mlflow.log_params(lgb_random.best_params_)\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"n_iter_random_search\", 20)\n",
    "    mlflow.log_param(\"device\", lgb_device)\n",
    "    \n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_scores_lgb.mean())\n",
    "    mlflow.log_metric(\"cv_auc_std\", cv_scores_lgb.std())\n",
    "    mlflow.log_metrics({\n",
    "        \"test_auc\": lgb_metrics['auc'],\n",
    "        \"test_accuracy\": lgb_metrics['accuracy'],\n",
    "        \"test_precision\": lgb_metrics['precision'],\n",
    "        \"test_recall\": lgb_metrics['recall'],\n",
    "        \"test_f1\": lgb_metrics['f1'],\n",
    "        \"optimal_threshold\": lgb_metrics['optimal_threshold'],\n",
    "        \"business_cost\": lgb_metrics['business_cost_optimal']\n",
    "    })\n",
    "    \n",
    "    mlflow.lightgbm.log_model(lgb_model, \"model\")\n",
    "    print(\"\\n‚úì Logged to MLflow!\")\n",
    "\n",
    "joblib.dump(lgb_model, '../models/lightgbm.pkl')\n",
    "print(\"‚úì Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Logistic Regression',\n",
    "        'AUC': lr_metrics['auc'],\n",
    "        'Accuracy': lr_metrics['accuracy'],\n",
    "        'Precision': lr_metrics['precision'],\n",
    "        'Recall': lr_metrics['recall'],\n",
    "        'F1-Score': lr_metrics['f1'],\n",
    "        'Business Cost': lr_metrics['business_cost_optimal'],\n",
    "        'Optimal Threshold': lr_metrics['optimal_threshold']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'AUC': rf_metrics['auc'],\n",
    "        'Accuracy': rf_metrics['accuracy'],\n",
    "        'Precision': rf_metrics['precision'],\n",
    "        'Recall': rf_metrics['recall'],\n",
    "        'F1-Score': rf_metrics['f1'],\n",
    "        'Business Cost': rf_metrics['business_cost_optimal'],\n",
    "        'Optimal Threshold': rf_metrics['optimal_threshold']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'AUC': xgb_metrics['auc'],\n",
    "        'Accuracy': xgb_metrics['accuracy'],\n",
    "        'Precision': xgb_metrics['precision'],\n",
    "        'Recall': xgb_metrics['recall'],\n",
    "        'F1-Score': xgb_metrics['f1'],\n",
    "        'Business Cost': xgb_metrics['business_cost_optimal'],\n",
    "        'Optimal Threshold': xgb_metrics['optimal_threshold']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'LightGBM',\n",
    "        'AUC': lgb_metrics['auc'],\n",
    "        'Accuracy': lgb_metrics['accuracy'],\n",
    "        'Precision': lgb_metrics['precision'],\n",
    "        'Recall': lgb_metrics['recall'],\n",
    "        'F1-Score': lgb_metrics['f1'],\n",
    "        'Business Cost': lgb_metrics['business_cost_optimal'],\n",
    "        'Optimal Threshold': lgb_metrics['optimal_threshold']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Select champion model\n",
    "best_model_idx = comparison_df['Business Cost'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"\\nüèÜ CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"   Business Cost: {comparison_df.loc[best_model_idx, 'Business Cost']:.2f}\")\n",
    "print(f\"   AUC: {comparison_df.loc[best_model_idx, 'AUC']:.4f}\")\n",
    "print(f\"   Optimal Threshold: {comparison_df.loc[best_model_idx, 'Optimal Threshold']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs('../reports', exist_ok=True)\n",
    "comparison_df.to_csv('../reports/model_comparison.csv', index=False)\n",
    "print(\"\\n‚úì Results saved to ../reports/model_comparison.csv\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# AUC\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['AUC'], color='steelblue')\n",
    "axes[0, 0].set_xlabel('AUC')\n",
    "axes[0, 0].set_title('Model Comparison - AUC')\n",
    "axes[0, 0].set_xlim([0.7, 0.85])\n",
    "\n",
    "# F1-Score\n",
    "axes[0, 1].barh(comparison_df['Model'], comparison_df['F1-Score'], color='forestgreen')\n",
    "axes[0, 1].set_xlabel('F1-Score')\n",
    "axes[0, 1].set_title('Model Comparison - F1-Score')\n",
    "\n",
    "# Business Cost (lower is better)\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['Business Cost'], color='tomato')\n",
    "axes[1, 0].set_xlabel('Business Cost (Lower is Better)')\n",
    "axes[1, 0].set_title('Model Comparison - Business Cost')\n",
    "axes[1, 0].invert_xaxis()\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].barh(comparison_df['Model'], comparison_df['Recall'], color='purple')\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_title('Model Comparison - Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Comparison plot saved to ../reports/figures/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples:      {X_train.shape[0]:,}\")\n",
    "print(f\"Test samples:          {X_test.shape[0]:,}\")\n",
    "print(f\"Features:              {X_train.shape[1]:,}\")\n",
    "print(f\"Class imbalance:       {scale_pos_weight:.2f}:1\")\n",
    "print(f\"\\nModels trained:        4\")\n",
    "print(f\"MLflow runs:           4\")\n",
    "print(f\"MLflow server:         {mlflow_server_uri}\")\n",
    "print(f\"\\nBest model:            {best_model_name}\")\n",
    "print(f\"Best business cost:    {comparison_df.loc[best_model_idx, 'Business Cost']:.2f}\")\n",
    "print(f\"Device used:           {'Apple Silicon (optimized CPU)' if is_apple_silicon else 'CPU'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"\\nüìä View results: {mlflow_server_uri}\")\n",
    "print(\"üìÅ Models saved in: ../models/\")\n",
    "print(\"üìà Reports saved in: ../reports/\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Run 03_explainability.ipynb for SHAP analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
