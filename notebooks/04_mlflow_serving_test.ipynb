{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Model Serving Test\n",
    "\n",
    "This notebook:\n",
    "1. Registers the champion model in MLflow Model Registry\n",
    "2. Transitions it to Production stage\n",
    "3. Tests the MLflow Model Serving API\n",
    "4. Benchmarks performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.533446Z",
     "start_time": "2025-12-13T18:34:32.286999Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "MLflow version: 3.7.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure MLflow"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.557388Z",
     "start_time": "2025-12-13T18:34:32.534512Z"
    }
   },
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment_name = \"home_credit_default_risk\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {experiment_name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://localhost:5000\n",
      "Experiment: home_credit_default_risk\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Champion Model Run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.577145Z",
     "start_time": "2025-12-13T18:34:32.558486Z"
    }
   },
   "source": [
    "# Load comparison results\n",
    "comparison_df = pd.read_csv('../reports/model_comparison.csv')\n",
    "best_model_idx = comparison_df['Business Cost'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"Champion Model: {best_model_name}\")\n",
    "print(f\"Business Cost: {comparison_df.loc[best_model_idx, 'Business Cost']:.2f}\")\n",
    "print(f\"AUC: {comparison_df.loc[best_model_idx, 'AUC']:.4f}\")\n",
    "\n",
    "# Map model name to run name\n",
    "run_name_map = {\n",
    "    'Logistic Regression': 'logistic_regression_baseline',\n",
    "    'Random Forest': 'random_forest_tuned',\n",
    "    'XGBoost': 'xgboost_tuned',\n",
    "    'LightGBM': 'lightgbm_tuned'\n",
    "}\n",
    "\n",
    "champion_run_name = run_name_map[best_model_name]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champion Model: LightGBM\n",
      "Business Cost: 4959.00\n",
      "AUC: 0.7793\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get All Runs from Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.609274Z",
     "start_time": "2025-12-13T18:34:32.578966Z"
    }
   },
   "source": [
    "# Get experiment\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search for all runs\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.business_cost ASC\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(runs)} runs in experiment:\")\n",
    "print(\"=\"*80)\n",
    "for run in runs:\n",
    "    run_name = run.data.tags.get('mlflow.runName', 'Unknown')\n",
    "    auc = run.data.metrics.get('test_auc', 0)\n",
    "    business_cost = run.data.metrics.get('business_cost', 0)\n",
    "    print(f\"{run_name:30s} | AUC: {auc:.4f} | Business Cost: {business_cost:.2f} | Run ID: {run.info.run_id[:8]}...\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 runs in experiment:\n",
      "================================================================================\n",
      "lightgbm_tuned                 | AUC: 0.7793 | Business Cost: 4959.00 | Run ID: 05452ecb...\n",
      "random_forest_tuned            | AUC: 0.7553 | Business Cost: 4964.00 | Run ID: 3419456e...\n",
      "xgboost_tuned                  | AUC: 0.7695 | Business Cost: 4965.00 | Run ID: c768a4de...\n",
      "logistic_regression_baseline   | AUC: 0.7684 | Business Cost: 4965.00 | Run ID: 8e84e916...\n",
      "logistic_regression_baseline   | AUC: 0.7684 | Business Cost: 4965.00 | Run ID: 2f886076...\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register Champion Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.686482Z",
     "start_time": "2025-12-13T18:34:32.614542Z"
    }
   },
   "source": [
    "# Find the champion run\n",
    "champion_run = None\n",
    "for run in runs:\n",
    "    run_name = run.data.tags.get('mlflow.runName', '')\n",
    "    if champion_run_name in run_name:\n",
    "        champion_run = run\n",
    "        break\n",
    "\n",
    "if champion_run is None:\n",
    "    raise ValueError(f\"Could not find run with name: {champion_run_name}\")\n",
    "\n",
    "champion_run_id = champion_run.info.run_id\n",
    "print(f\"Champion Run ID: {champion_run_id}\")\n",
    "\n",
    "# Register model\n",
    "model_name = \"home_credit_scoring\"\n",
    "model_uri = f\"runs:/{champion_run_id}/model\"\n",
    "\n",
    "try:\n",
    "    # Try to register model\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"\\nModel registered successfully!\")\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    print(f\"Version: {model_version.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model already registered or error: {e}\")\n",
    "    # Get latest version\n",
    "    model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    if model_versions:\n",
    "        model_version = model_versions[0]\n",
    "        print(f\"Using existing model version: {model_version.version}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'home_credit_scoring'.\n",
      "2025/12/13 19:34:32 WARNING mlflow.tracking._model_registry.fluent: Run with id 05452ecb1daa4fd09593e3119bd07591 has no artifacts at artifact path 'model', registering model based on models:/m-7c7155d84f3a42d5aac70ead4378bbc1 instead\n",
      "2025/12/13 19:34:32 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: home_credit_scoring, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champion Run ID: 05452ecb1daa4fd09593e3119bd07591\n",
      "\n",
      "Model registered successfully!\n",
      "Model Name: home_credit_scoring\n",
      "Version: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'home_credit_scoring'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transition Model to Production"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.723900Z",
     "start_time": "2025-12-13T18:34:32.695276Z"
    }
   },
   "source": [
    "# Transition to Production stage\n",
    "try:\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"Model transitioned to Production stage!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error transitioning model: {e}\")\n",
    "\n",
    "# Verify\n",
    "prod_model = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
    "if prod_model:\n",
    "    print(f\"\\nProduction model: {model_name} v{prod_model[0].version}\")\n",
    "    print(f\"Status: {prod_model[0].status}\")\n",
    "    print(f\"Run ID: {prod_model[0].run_id}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model transitioned to Production stage!\n",
      "\n",
      "Production model: home_credit_scoring v1\n",
      "Status: READY\n",
      "Run ID: 05452ecb1daa4fd09593e3119bd07591\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:32.740429Z",
     "start_time": "2025-12-13T18:34:32.724946Z"
    }
   },
   "source": [
    "# Get model details\n",
    "model_details = client.get_model_version(model_name, model_version.version)\n",
    "\n",
    "print(\"\\nModel Metadata:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Name: {model_details.name}\")\n",
    "print(f\"Version: {model_details.version}\")\n",
    "print(f\"Stage: {model_details.current_stage}\")\n",
    "print(f\"Run ID: {model_details.run_id}\")\n",
    "print(f\"Source: {model_details.source}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Metadata:\n",
      "============================================================\n",
      "Name: home_credit_scoring\n",
      "Version: 1\n",
      "Stage: Production\n",
      "Run ID: 05452ecb1daa4fd09593e3119bd07591\n",
      "Source: models:/m-7c7155d84f3a42d5aac70ead4378bbc1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start MLflow Model Server (Instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start the MLflow Model Serving server:\n",
    "\n",
    "Open a **new terminal** and run:\n",
    "\n",
    "```bash\n",
    "cd \"Projet Final\"\n",
    "mlflow models serve -m \"models:/home_credit_scoring/Production\" -p 5001 --env-manager local\n",
    "```\n",
    "\n",
    "Wait for the message: **\"Listening at: http://127.0.0.1:5001\"**\n",
    "\n",
    "Then continue with the cells below to test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Server Health"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:34:58.851269Z",
     "start_time": "2025-12-13T18:34:32.741514Z"
    }
   },
   "source": [
    "# Wait for server to be ready\n",
    "server_url = \"http://127.0.0.1:5001\"\n",
    "health_url = f\"{server_url}/health\"\n",
    "invocations_url = f\"{server_url}/invocations\"\n",
    "\n",
    "print(\"Checking if server is ready...\")\n",
    "max_retries = 30\n",
    "retry_delay = 2\n",
    "\n",
    "for i in range(max_retries):\n",
    "    try:\n",
    "        response = requests.get(health_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ Server is ready!\")\n",
    "            print(f\"Health check response: {response.text}\")\n",
    "            break\n",
    "    except:\n",
    "        if i < max_retries - 1:\n",
    "            print(f\"Waiting for server... ({i+1}/{max_retries})\")\n",
    "            time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"✗ Server not responding. Make sure you started the server in terminal.\")\n",
    "            print(\"Run: mlflow models serve -m 'models:/home_credit_scoring/Production' -p 5001 --env-manager local\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if server is ready...\n",
      "Waiting for server... (1/30)\n",
      "Waiting for server... (2/30)\n",
      "Waiting for server... (3/30)\n",
      "Waiting for server... (4/30)\n",
      "Waiting for server... (5/30)\n",
      "Waiting for server... (6/30)\n",
      "Waiting for server... (7/30)\n",
      "Waiting for server... (8/30)\n",
      "Waiting for server... (9/30)\n",
      "Waiting for server... (10/30)\n",
      "Waiting for server... (11/30)\n",
      "Waiting for server... (12/30)\n",
      "Waiting for server... (13/30)\n",
      "✓ Server is ready!\n",
      "Health check response: \n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:02.907545Z",
     "start_time": "2025-12-13T18:34:58.889848Z"
    }
   },
   "source": [
    "# Load prepared data\n",
    "df = pd.read_csv('../data/application_train_prepared.csv')\n",
    "\n",
    "# Separate features\n",
    "if 'TARGET' in df.columns:\n",
    "    target_col = 'TARGET'\n",
    "else:\n",
    "    target_col = [col for col in df.columns if 'target' in col.lower()][0]\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Get a few test samples\n",
    "test_samples = X.sample(n=5, random_state=42)\n",
    "test_targets = y.loc[test_samples.index]\n",
    "\n",
    "print(f\"Selected {len(test_samples)} test samples\")\n",
    "print(f\"Actual labels: {test_targets.values}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 test samples\n",
      "Actual labels: [0 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prepare Request Payload"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:02.930932Z",
     "start_time": "2025-12-13T18:35:02.920477Z"
    }
   },
   "source": [
    "# Format data for MLflow (dataframe_split format)\n",
    "payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": test_samples.columns.tolist(),\n",
    "        \"data\": test_samples.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Payload prepared:\")\n",
    "print(f\"Number of columns: {len(payload['dataframe_split']['columns'])}\")\n",
    "print(f\"Number of samples: {len(payload['dataframe_split']['data'])}\")\n",
    "print(f\"\\nFirst few columns: {payload['dataframe_split']['columns'][:5]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload prepared:\n",
      "Number of columns: 335\n",
      "Number of samples: 5\n",
      "\n",
      "First few columns: ['SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Send Prediction Request"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:02.958187Z",
     "start_time": "2025-12-13T18:35:02.932283Z"
    }
   },
   "source": [
    "# Send POST request\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "print(\"Sending prediction request...\")\n",
    "start_time = time.time()\n",
    "\n",
    "response = requests.post(\n",
    "    invocations_url,\n",
    "    json=payload,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Response Status: {response.status_code}\")\n",
    "print(f\"Response Time: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    predictions = response.json()['predictions']\n",
    "    print(f\"\\n✓ Predictions received successfully!\")\n",
    "    print(f\"Number of predictions: {len(predictions)}\")\n",
    "else:\n",
    "    print(f\"✗ Error: {response.text}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prediction request...\n",
      "Response Status: 200\n",
      "Response Time: 0.011 seconds\n",
      "\n",
      "✓ Predictions received successfully!\n",
      "Number of predictions: 5\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Parse and Display Predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:02.969978Z",
     "start_time": "2025-12-13T18:35:02.958825Z"
    }
   },
   "source": [
    "# Display predictions vs actual\n",
    "if response.status_code == 200:\n",
    "    results_df = pd.DataFrame({\n",
    "        'Index': test_samples.index,\n",
    "        'Actual': test_targets.values,\n",
    "        'Predicted_Proba': [pred[1] if isinstance(pred, list) else pred for pred in predictions],\n",
    "        'Predicted_Class': [(pred[1] if isinstance(pred, list) else pred) >= 0.5 for pred in predictions]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check accuracy\n",
    "    correct = (results_df['Actual'] == results_df['Predicted_Class']).sum()\n",
    "    accuracy = correct / len(results_df)\n",
    "    print(f\"\\nAccuracy on test samples: {accuracy:.2%} ({correct}/{len(results_df)})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICTION RESULTS\n",
      "================================================================================\n",
      " Index  Actual  Predicted_Proba  Predicted_Class\n",
      "245895       0                1             True\n",
      " 98194       0                0            False\n",
      " 36463       0                1             True\n",
      "249923       0                0            False\n",
      "158389       0                0            False\n",
      "================================================================================\n",
      "\n",
      "Accuracy on test samples: 60.00% (3/5)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:02.998667Z",
     "start_time": "2025-12-13T18:35:02.970941Z"
    }
   },
   "source": [
    "# Test with larger batch\n",
    "batch_size = 100\n",
    "batch_samples = X.sample(n=batch_size, random_state=123)\n",
    "\n",
    "batch_payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": batch_samples.columns.tolist(),\n",
    "        \"data\": batch_samples.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Sending batch request ({batch_size} samples)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "batch_response = requests.post(\n",
    "    invocations_url,\n",
    "    json=batch_payload,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Response Status: {batch_response.status_code}\")\n",
    "print(f\"Response Time: {elapsed_time:.3f} seconds\")\n",
    "print(f\"Throughput: {batch_size / elapsed_time:.2f} predictions/second\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending batch request (100 samples)...\n",
      "Response Status: 200\n",
      "Response Time: 0.015 seconds\n",
      "Throughput: 6892.29 predictions/second\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Edge Cases Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:03.028116Z",
     "start_time": "2025-12-13T18:35:02.999642Z"
    }
   },
   "source": [
    "# Test with edge cases\n",
    "print(\"Testing edge cases...\\n\")\n",
    "\n",
    "# Test 1: Single sample\n",
    "single_sample = X.iloc[[0]]\n",
    "single_payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": single_sample.columns.tolist(),\n",
    "        \"data\": single_sample.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "response1 = requests.post(invocations_url, json=single_payload, headers=headers)\n",
    "print(f\"✓ Single sample test: Status {response1.status_code}\")\n",
    "\n",
    "# Test 2: Sample with potential outliers\n",
    "# Get samples with extreme values\n",
    "outlier_samples = X.sample(n=3, random_state=999)\n",
    "outlier_payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": outlier_samples.columns.tolist(),\n",
    "        \"data\": outlier_samples.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "response2 = requests.post(invocations_url, json=outlier_payload, headers=headers)\n",
    "print(f\"✓ Outlier samples test: Status {response2.status_code}\")\n",
    "\n",
    "print(\"\\nAll edge case tests passed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing edge cases...\n",
      "\n",
      "✓ Single sample test: Status 200\n",
      "✓ Outlier samples test: Status 200\n",
      "\n",
      "All edge case tests passed!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:03.271086Z",
     "start_time": "2025-12-13T18:35:03.028954Z"
    }
   },
   "source": [
    "# Benchmark with multiple requests\n",
    "print(\"Running performance benchmark...\")\n",
    "n_requests = 50\n",
    "request_times = []\n",
    "\n",
    "benchmark_sample = X.sample(n=10, random_state=42)\n",
    "benchmark_payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": benchmark_sample.columns.tolist(),\n",
    "        \"data\": benchmark_sample.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in range(n_requests):\n",
    "    start = time.time()\n",
    "    response = requests.post(invocations_url, json=benchmark_payload, headers=headers)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        request_times.append(elapsed)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Progress: {i+1}/{n_requests}\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_time = np.mean(request_times)\n",
    "median_time = np.median(request_times)\n",
    "p95_time = np.percentile(request_times, 95)\n",
    "p99_time = np.percentile(request_times, 99)\n",
    "throughput = 1 / mean_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of requests: {n_requests}\")\n",
    "print(f\"Samples per request: {len(benchmark_sample)}\")\n",
    "print(f\"\\nLatency:\")\n",
    "print(f\"  Mean: {mean_time*1000:.2f} ms\")\n",
    "print(f\"  Median: {median_time*1000:.2f} ms\")\n",
    "print(f\"  P95: {p95_time*1000:.2f} ms\")\n",
    "print(f\"  P99: {p99_time*1000:.2f} ms\")\n",
    "print(f\"\\nThroughput: {throughput:.2f} requests/second\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running performance benchmark...\n",
      "Progress: 10/50\n",
      "Progress: 20/50\n",
      "Progress: 30/50\n",
      "Progress: 40/50\n",
      "Progress: 50/50\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE BENCHMARK RESULTS\n",
      "============================================================\n",
      "Number of requests: 50\n",
      "Samples per request: 10\n",
      "\n",
      "Latency:\n",
      "  Mean: 4.65 ms\n",
      "  Median: 4.57 ms\n",
      "  P95: 5.33 ms\n",
      "  P99: 5.63 ms\n",
      "\n",
      "Throughput: 215.13 requests/second\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. API Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Endpoint\n",
    "\n",
    "**URL**: `http://127.0.0.1:5001/invocations`\n",
    "\n",
    "**Method**: `POST`\n",
    "\n",
    "**Headers**:\n",
    "```\n",
    "Content-Type: application/json\n",
    "```\n",
    "\n",
    "### Request Format\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"dataframe_split\": {\n",
    "    \"columns\": [\"feature1\", \"feature2\", ...],\n",
    "    \"data\": [\n",
    "      [value1, value2, ...],\n",
    "      [value1, value2, ...]\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Response Format\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    [prob_class_0, prob_class_1],\n",
    "    [prob_class_0, prob_class_1]\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### cURL Example\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:5001/invocations \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d @sample_request.json\n",
    "```\n",
    "\n",
    "### Python Example\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:5001/invocations\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [...],\n",
    "        \"data\": [[...]]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "predictions = response.json()['predictions']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Create Sample Request File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:35:03.277963Z",
     "start_time": "2025-12-13T18:35:03.271676Z"
    }
   },
   "source": [
    "# Create sample request JSON file\n",
    "sample_request = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": test_samples.columns.tolist(),\n",
    "        \"data\": test_samples.iloc[:1].values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../sample_request.json', 'w') as f:\n",
    "    json.dump(sample_request, f, indent=2)\n",
    "\n",
    "print(\"Sample request file created: ../sample_request.json\")\n",
    "print(\"\\nYou can test with:\")\n",
    "print(\"curl -X POST http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d @sample_request.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample request file created: ../sample_request.json\n",
      "\n",
      "You can test with:\n",
      "curl -X POST http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d @sample_request.json\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Registered the champion model in MLflow Model Registry\n",
    "2. ✅ Transitioned model to Production stage\n",
    "3. ✅ Tested the MLflow serving API with various payloads\n",
    "4. ✅ Benchmarked API performance\n",
    "5. ✅ Created API documentation and sample requests\n",
    "\n",
    "### Model Registry Status:\n",
    "- **Model Name**: home_credit_scoring\n",
    "- **Stage**: Production\n",
    "- **Model Type**: {best_model_name}\n",
    "\n",
    "### API Performance:\n",
    "- Ready for production deployment\n",
    "- Consistent response times\n",
    "- Handles batch predictions efficiently\n",
    "\n",
    "### Next Steps:\n",
    "1. Dockerize the application (Phase 3)\n",
    "2. Deploy to production environment\n",
    "3. Set up monitoring and alerting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
