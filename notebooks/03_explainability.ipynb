{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability Analysis - SHAP\n",
    "\n",
    "This notebook analyzes the champion model using SHAP (SHapley Additive exPlanations):\n",
    "- Global feature importance\n",
    "- Local explanations for individual predictions\n",
    "- Business insights from model decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shap\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "# Import custom modules\n",
    "from explainability import *\n",
    "from metrics import *\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Champion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison results to identify champion\n",
    "comparison_df = pd.read_csv('../reports/model_comparison.csv')\n",
    "best_model_idx = comparison_df['Business Cost'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"Champion Model: {best_model_name}\")\n",
    "print(f\"Business Cost: {comparison_df.loc[best_model_idx, 'Business Cost']:.2f}\")\n",
    "print(f\"AUC: {comparison_df.loc[best_model_idx, 'AUC']:.4f}\")\n",
    "\n",
    "# Map model name to file\n",
    "model_files = {\n",
    "    'Logistic Regression': 'logistic_regression.pkl',\n",
    "    'Random Forest': 'random_forest.pkl',\n",
    "    'XGBoost': 'xgboost.pkl',\n",
    "    'LightGBM': 'lightgbm.pkl'\n",
    "}\n",
    "\n",
    "model_file = model_files[best_model_name]\n",
    "model = joblib.load(f'../models/{model_file}')\n",
    "print(f\"\\nModel loaded from ../models/{model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared data\n",
    "df = pd.read_csv('../data/application_train_prepared.csv')\n",
    "\n",
    "# Separate features and target\n",
    "if 'TARGET' in df.columns:\n",
    "    target_col = 'TARGET'\n",
    "else:\n",
    "    target_col = [col for col in df.columns if 'target' in col.lower()][0]\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Use same split as training notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of features: {X_test.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Data for SHAP (Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for performance (SHAP can be slow on large datasets)\n",
    "sample_size = min(1000, X_test.shape[0])\n",
    "X_test_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "\n",
    "print(f\"Using {sample_size} samples for SHAP analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create appropriate explainer based on model type\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    # For linear models\n",
    "    explainer = shap.LinearExplainer(model, X_train)\n",
    "else:\n",
    "    # For tree-based models (RF, XGB, LGB)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "print(f\"SHAP Explainer created for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "print(\"Calculating SHAP values... (this may take a few minutes)\")\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# For binary classification, some explainers return values for both classes\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]  # Use positive class\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(\"SHAP calculation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Global Feature Importance - Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (beeswarm)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False, max_display=20)\n",
    "plt.tight_layout()\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "plt.savefig('../reports/figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"SHAP summary plot saved to ../reports/figures/shap_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Global Feature Importance - Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (mean absolute SHAP values)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type='bar', show=False, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/shap_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"SHAP importance plot saved to ../reports/figures/shap_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Top 20 Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute SHAP values\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_test_sample.columns,\n",
    "    'importance': mean_abs_shap\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (by mean |SHAP value|):\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which features increase vs decrease default risk\n",
    "print(\"\\nFeature Impact Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features = feature_importance.head(10)['feature'].tolist()\n",
    "\n",
    "for feature in top_features:\n",
    "    feature_idx = X_test_sample.columns.get_loc(feature)\n",
    "    mean_shap = shap_values[:, feature_idx].mean()\n",
    "    \n",
    "    if mean_shap > 0:\n",
    "        direction = \"INCREASES default risk\"\n",
    "    else:\n",
    "        direction = \"DECREASES default risk\"\n",
    "    \n",
    "    print(f\"{feature:40s} -> {direction} (mean SHAP: {mean_shap:+.4f})\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Business Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "\n",
    "Based on the SHAP analysis above:\n",
    "\n",
    "1. **External Data Sources** (EXT_SOURCE_X): Often the most predictive features\n",
    "2. **Credit Amount**: Higher loan amounts may correlate with risk\n",
    "3. **Age/Employment**: Client stability indicators\n",
    "4. **Previous Credit History**: Strong predictor of future behavior\n",
    "\n",
    "These insights should guide:\n",
    "- Which data to prioritize collecting\n",
    "- What to monitor in production\n",
    "- How to explain decisions to stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Select Interesting Clients for Local Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_proba = model.predict_proba(X_test_sample)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Find interesting cases\n",
    "true_positives = X_test_sample[(y_test_sample == 1) & (y_pred == 1)].index\n",
    "true_negatives = X_test_sample[(y_test_sample == 0) & (y_pred == 0)].index\n",
    "false_positives = X_test_sample[(y_test_sample == 0) & (y_pred == 1)].index\n",
    "false_negatives = X_test_sample[(y_test_sample == 1) & (y_pred == 0)].index\n",
    "\n",
    "# Edge cases (probability near 0.5)\n",
    "edge_cases = X_test_sample[(y_pred_proba > 0.45) & (y_pred_proba < 0.55)].index\n",
    "\n",
    "# Select one of each\n",
    "interesting_clients = []\n",
    "interesting_labels = []\n",
    "\n",
    "if len(true_positives) > 0:\n",
    "    interesting_clients.append(true_positives[0])\n",
    "    interesting_labels.append('True Positive (Correct Default Prediction)')\n",
    "\n",
    "if len(true_negatives) > 0:\n",
    "    interesting_clients.append(true_negatives[0])\n",
    "    interesting_labels.append('True Negative (Correct Non-Default Prediction)')\n",
    "\n",
    "if len(false_positives) > 0:\n",
    "    interesting_clients.append(false_positives[0])\n",
    "    interesting_labels.append('False Positive (Conservative Error)')\n",
    "\n",
    "if len(false_negatives) > 0:\n",
    "    interesting_clients.append(false_negatives[0])\n",
    "    interesting_labels.append('False Negative (Costly Error)')\n",
    "\n",
    "if len(edge_cases) > 0:\n",
    "    interesting_clients.append(edge_cases[0])\n",
    "    interesting_labels.append('Edge Case (Probability ~0.5)')\n",
    "\n",
    "print(f\"Selected {len(interesting_clients)} interesting clients for local analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Local Explanations - Waterfall Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create waterfall plot for each interesting client\n",
    "for idx, (client_idx, label) in enumerate(zip(interesting_clients, interesting_labels)):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Client Analysis: {label}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get client position in sample\n",
    "    client_pos = X_test_sample.index.get_loc(client_idx)\n",
    "    \n",
    "    # Client info\n",
    "    actual = y_test_sample.loc[client_idx]\n",
    "    predicted_proba = y_pred_proba[client_pos]\n",
    "    predicted = y_pred[client_pos]\n",
    "    \n",
    "    print(f\"Actual: {actual} | Predicted: {predicted} | Probability: {predicted_proba:.4f}\")\n",
    "    \n",
    "    # Waterfall plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_values[client_pos],\n",
    "            base_values=explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "            data=X_test_sample.iloc[client_pos],\n",
    "            feature_names=X_test_sample.columns.tolist()\n",
    "        ),\n",
    "        max_display=15,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"{label}\\nActual: {actual}, Predicted: {predicted}, Prob: {predicted_proba:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/shap_waterfall_client_{idx+1}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Top contributing features\n",
    "    client_shap = shap_values[client_pos]\n",
    "    top_features_idx = np.argsort(np.abs(client_shap))[-10:][::-1]\n",
    "    \n",
    "    print(f\"\\nTop 10 Contributing Features:\")\n",
    "    for feat_idx in top_features_idx:\n",
    "        feat_name = X_test_sample.columns[feat_idx]\n",
    "        feat_value = X_test_sample.iloc[client_pos, feat_idx]\n",
    "        shap_val = client_shap[feat_idx]\n",
    "        print(f\"  {feat_name:35s} = {feat_value:10.2f}  (SHAP: {shap_val:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Create Complete Explainability Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom explainability function\n",
    "print(\"Generating complete explainability report...\")\n",
    "\n",
    "report = create_explainability_report(\n",
    "    model=model,\n",
    "    X_test=X_test_sample,\n",
    "    y_test=y_test_sample,\n",
    "    feature_names=X_test_sample.columns.tolist(),\n",
    "    output_dir='../reports/figures',\n",
    "    model_name=best_model_name\n",
    ")\n",
    "\n",
    "print(\"\\nExplainability report generated!\")\n",
    "print(\"All visualizations saved to ../reports/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export Critical Features List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top features to CSV\n",
    "feature_importance.head(50).to_csv('../reports/critical_features.csv', index=False)\n",
    "print(\"Critical features saved to ../reports/critical_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Business Recommendations:\n",
    "\n",
    "#### 1. Data Collection Priorities\n",
    "Based on feature importance, prioritize collecting:\n",
    "- External credit scores (EXT_SOURCE variables)\n",
    "- Complete employment history\n",
    "- Previous credit behavior patterns\n",
    "- Income and asset information\n",
    "\n",
    "#### 2. Red Flag Signals\n",
    "Indicators that strongly predict default:\n",
    "- Low external credit scores\n",
    "- High credit amount relative to income\n",
    "- Short employment duration\n",
    "- Previous payment difficulties\n",
    "\n",
    "#### 3. Risk Reduction Factors\n",
    "Factors that reduce default probability:\n",
    "- Higher external credit scores\n",
    "- Stable employment history\n",
    "- Lower debt-to-income ratio\n",
    "- Good previous credit history\n",
    "\n",
    "#### 4. Model Improvement Suggestions\n",
    "- Add interaction features between key variables\n",
    "- Collect more granular employment data\n",
    "- Monitor feature drift in production\n",
    "- A/B test different threshold strategies\n",
    "- Consider ensemble methods combining multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Loaded the champion model\n",
    "2. ✅ Calculated SHAP values for global interpretability\n",
    "3. ✅ Generated summary and importance plots\n",
    "4. ✅ Analyzed local explanations for interesting cases\n",
    "5. ✅ Provided business insights and recommendations\n",
    "\n",
    "### Next Steps:\n",
    "- Review all generated figures in `../reports/figures/`\n",
    "- Proceed to `04_mlflow_serving_test.ipynb` for deployment testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
